# -*- coding: utf-8 -*-
"""SalmonEnvParameterTests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vysZTwq6MYCzPBkz2oAdpKrIcO6ift9

<a target="_blank" href="https://colab.research.google.com/github/cboettig/rl-minicourse/blob/main/challenge.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

#  Conservation Gym Testbed

In this example, we set up a generic three species, two action problem, and illustrate how to provide a custom population dynamics function, action function, and utility function to represent a caribou conservation objective.
"""

# we'll need these packages to begin
import stable-baselines3 
import plotnine 
import polars 
import sb3_contrib 
import tensorboard

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1" # change to -1 if you want to use CPU

import numpy as np
# pop = seals, lamprey, salmon
# Salmon Scenario
def dynamics(pop, effort, population_parameter, harvest_fn, p, timestep=1):

    pop = harvest_fn(pop, effort)
    X, Y, Z = pop[0], pop[1], pop[2]

    K = p["K"] + population_parameter # - 0.2 * np.sin(2 * np.pi * timestep / 3200)
    D = p["D"]

    X += (p["r_x"] * X * (1 - (X + p["tau_xy"] * Y) / K)
            - (1 - D) * Z * (X**2) / (p["v0"]**2 + X**2)
            + p["sigma_x"] * X * np.random.normal()
            )

    Y += (p["r_y"] * Y * (1 - (Y + p["tau_yx"]* X ) / K )
            - D * Z * (Y**2) / (p["v0"]**2 + Y**2)
            + p["sigma_y"] * Y * np.random.normal()
            )

    Z += p["alpha"] * Z * (
            (1-D) * (X**2) / (p["v0"]**2 + X**2)
            + D * (Y**2) / (p["v0"]**2 + Y**2)
            ) - p["dH"] * Z +  p["sigma_z"] * Z  * np.random.normal()

    pop = np.array([X, Y, Z], dtype=np.float32)
    pop = np.clip(pop, [0,0,0], [np.Inf, np.Inf, np.Inf])
    return(pop)

initial_pop = [0.5, 0.5, 0.2]


parameters = {
"r_x": np.float32(0.13),
"r_y": np.float32(0.2),
"K": np.float32(1),
"beta": np.float32(.1),
"v0":  np.float32(0.1),
"D": np.float32(0.8),
"tau_yx": np.float32(0.7),
"tau_xy": np.float32(0.2),
"alpha": np.float32(.4),
"dH": np.float32(0.03),
"sigma_x": np.float32(0.05),
"sigma_y": np.float32(0.05),
"sigma_z": np.float32(0.05)
}

"""We must also define the dynamics of the action, a 'harvest' or culling function.  In this scenario, we imagine that we can cull either the elk or wolf population (or both).  We assume our control action introduces a percent mortality equal to the control effort applied times a catachability coefficient:"""

def harvest(pop, effort):
    q0 = 0.5 # catchability coefficients -- erradication is impossible
    q2 = 0.5
    pop[0] = pop[0] * (1 - effort[0] * q0) # pop 0, seals
    pop[2] = pop[2] * (1 - effort[1] * q2) # pop 2, salmon
    return pop

"""Lastly, we need to define the utility or reward derived from taking these actions under this population state.  In this scenario, our population control actions are costly, while we acrue a benefit proportional to the size of the current caribou population:"""

def utility(pop, effort):
    benefits = 0.5 * pop[1] # benefit from Salmon
    costs = .00001 * (effort[0] + effort[1]) # cost to culling
    if np.any(pop <= 0.01):
        benefits -= 1
    return benefits - costs

"""To simulate our environment and allow RL algorithms to train on this environment, we define a simple python class using the gym module.  This class defines the possible action space as two continuously-valued action variables (culling effort of elk and wolves respectively), and three continuously valued state variables (population of elk, caribou and wolves).  To improve performance of RL training, it is necessary to transform the continuous space to -1, 1"""

import gymnasium as gym

class s3a2(gym.Env):
    """A 3-species ecosystem model with two control actions"""
    def __init__(self, config=None):
        config = config or {}

        ## these parameters may be specified in config
        self.Tmax = config.get("Tmax", 800)
        self.threshold = config.get("threshold", np.float32(1e-4))
        self.init_sigma = config.get("init_sigma", np.float32(1e-3))
        self.training = config.get("training", True)
        self.initial_pop = config.get("initial_pop", initial_pop)
        self.parameters = config.get("parameters", parameters)
        self.dynamics = config.get("dynamics", dynamics)
        self.harvest = config.get("harvest", harvest)
        self.utility = config.get("utility", utility)
        self.observe = config.get("observe", lambda state: state) # default to perfectly observed case
        self.bound = 2 * self.parameters["K"]

        self.action_space = gym.spaces.Box(
            np.array([-1, -1, 0], dtype=np.float32),
            np.array([1, 1, 0.1], dtype=np.float32),
            dtype = np.float32
        )
        self.observation_space = gym.spaces.Box(
            np.array([-1, -1, -1], dtype=np.float32),
            np.array([1, 1, 1], dtype=np.float32),
            dtype=np.float32,
        )
        self.reset(seed=config.get("seed", None))


    def reset(self, *, seed=None, options=None):
        self.timestep = 0
        self.initial_pop += np.multiply(self.initial_pop, np.float32(self.init_sigma * np.random.normal(size=3)))
        self.state = self.state_units(self.initial_pop)
        info = {}
        return self.observe(self.state), info


    def step(self, action):
        action = np.clip(action, self.action_space.low, self.action_space.high)
        pop = self.population_units(self.state) # current state in natural units
        human_actions = action[:2]
        pop_param = action[2]
        effort = (human_actions + 1.) / 2

        # harvest and recruitment
        reward = self.utility(pop, effort)
        nextpop = self.dynamics(pop, effort, pop_param, self.harvest, self.parameters, self.timestep)

        self.timestep += 1
        terminated = bool(self.timestep > self.Tmax)

        # in training mode only: punish for population collapse
        if any(pop <= self.threshold) and self.training:
            terminated = True
            reward -= 50/self.timestep

        self.state = self.state_units(nextpop) # transform into [-1, 1] space
        observation = self.observe(self.state) # same as self.state
        return observation, reward, terminated, False, {}

    def state_units(self, pop):
        self.state = 2 * pop / self.bound - 1
        self.state = np.clip(self.state,
                             np.repeat(-1, self.state.__len__()),
                             np.repeat(1, self.state.__len__()))
        return np.float32(self.state)

    def population_units(self, state):
        pop = (state + 1) * self.bound /2
        return np.clip(pop,
                       np.repeat(0, pop.__len__()),
                       np.repeat(np.Inf, pop.__len__()))

# verify that the environment is defined correctly
from stable_baselines3.common.env_checker import check_env
env = s3a2()
check_env(env, warn=True)

# Attempt to Achive Quasi Equilibrium:

from scipy.optimize import minimize

# Obtain final populaiton proportions from polars dataframe
def get_final_pop(df):
  final_pops = df.select(["X", "Y", "Z"]).tail(1)
  return list(final_pops.to_numpy()[0])

# Metric to measure equilibrium between population proportions
def balance_pops(df):
  final_pops = get_final_pop(df)
  pop_avg = np.mean(final_pops)
  return np.mean((final_pops - pop_avg) ** 2)

# Run an episode with inputted action and return above metric
def quasi_equilibrium(action):
  df = []
  episode_reward = 0
  observation, _ = env.reset()
  for t in range(env.Tmax):
    obs = env.population_units(observation) # natural units
    df.append([t, episode_reward, *obs])
    observation, reward, terminated, done, info = env.step(action)
    episode_reward += reward

  df = pl.DataFrame(df, schema=["t", "reward", "X", "Y", "Z"])
  return balance_pops(df)

# Minimizes metric to obtain "optimal" action
optimal_action = minimize(quasi_equilibrium, [-1,-1,0], method="Nelder-Mead")["x"]

import polars as pl
from plotnine import ggplot, aes, geom_line

# Obtains the dataframe from running an episode with an action
def obtain_simulated_data(action):
  df = []
  episode_reward = 0
  observation, _ = env.reset()
  for t in range(env.Tmax):
    obs = env.population_units(observation) # natural units
    df.append([t, episode_reward, *obs])
    observation, reward, terminated, done, info = env.step(action)
    episode_reward += reward

  df = pl.DataFrame(df, schema=["t", "reward", "X", "Y", "Z"])
  return df

# Plots dataframe of episode
def plot_simulation(df):
  cols = ["t", "reward", "X", "Y", "Z"]

  dfl = (pl.DataFrame(df, schema=cols).
          select(["t", "X", "Y", "Z"]).
          melt("t")
        )
  p = ggplot(dfl, aes("t", "value", color="variable")) + geom_line()
  display(p)

# Runs entire simulation by obtaining simulated episode, plotting graph, and returning the population proportions
def run_simulation(action):
  df = obtain_simulated_data(action)
  plot_simulation(df)
  return get_final_pop(df)


run_simulation(optimal_action)